"""
Detec√ß√£o de Ve√≠culos Clonados usando Random Forest
==================================================

Prot√≥tipo para detec√ß√£o de ve√≠culos clonados com base em dados ALPR simulados.
Utiliza Random Forest para classificar pares de capturas como suspeitos ou n√£o.

Autor: Sistema de Detec√ß√£o ALPR
Data: 2025
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from haversine import haversine
from datetime import datetime
import warnings
import os
warnings.filterwarnings('ignore')

# =========================
# 1. Carregamento de dados ALPR
# =========================

def carregar_dados_alpr(caminho_arquivo="passagens.csv"):
    """
    Carrega dados ALPR de um arquivo CSV gerado pelo simulador.
    
    Args:
        caminho_arquivo: Caminho para o arquivo CSV das passagens
        
    Returns:
        DataFrame: Dados carregados ou None se houver erro
    """
    try:
        if not os.path.exists(caminho_arquivo):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {caminho_arquivo}")
        
        df = pd.read_csv(caminho_arquivo)
        print(f"‚úÖ Dados carregados: {len(df):,} passagens de {caminho_arquivo}")
        print(f"   üè∑Ô∏è Placas √∫nicas: {df['placa'].nunique():,}")
        print(f"   üì° C√¢meras: {df['cam'].nunique()}")
        
        # Verificar colunas obrigat√≥rias
        colunas_obrigatorias = ['placa', 'cam', 'timestamp', 'lat', 'lon']
        colunas_faltando = [col for col in colunas_obrigatorias if col not in df.columns]
        
        if colunas_faltando:
            raise ValueError(f"Colunas obrigat√≥rias n√£o encontradas: {colunas_faltando}")
        
        # Renomear timestamp para ts para compatibilidade
        if 'timestamp' in df.columns and 'ts' not in df.columns:
            df['ts'] = df['timestamp']
        
        # Converter timestamp para milissegundos se necess√°rio
        if df['ts'].dtype != 'int64':
            print("‚ö†Ô∏è Convertendo timestamps para milissegundos...")
            df['ts'] = pd.to_datetime(df['ts']).astype(int) // 10**6
        
        # Estat√≠sticas temporais
        inicio = pd.to_datetime(df['ts'], unit='ms').min()
        fim = pd.to_datetime(df['ts'], unit='ms').max()
        duracao = fim - inicio
        
        print(f"   üìÖ Per√≠odo: {inicio.strftime('%Y-%m-%d %H:%M')} at√© {fim.strftime('%Y-%m-%d %H:%M')}")
        print(f"   ‚è±Ô∏è Dura√ß√£o: {duracao.total_seconds()/3600:.1f} horas")
        
        return df
        
    except Exception as e:
        print(f"‚ùå Erro ao carregar dados ALPR: {e}")
        return None

def carregar_veiculos_info(caminho_arquivo="veiculos_gerados.csv"):
    """
    Carrega informa√ß√µes dos ve√≠culos para identificar quais s√£o clonados.
    
    Args:
        caminho_arquivo: Caminho para o arquivo de ve√≠culos
        
    Returns:
        DataFrame: Informa√ß√µes dos ve√≠culos ou None se n√£o encontrado
    """
    try:
        if not os.path.exists(caminho_arquivo):
            print(f"‚ö†Ô∏è Arquivo de ve√≠culos n√£o encontrado: {caminho_arquivo}")
            print("   Continuando sem informa√ß√£o de ve√≠culos clonados...")
            return None
        
        df_veiculos = pd.read_csv(caminho_arquivo)
        print(f"‚úÖ Informa√ß√µes de ve√≠culos carregadas: {len(df_veiculos):,} ve√≠culos")
        
        if 'clonado' in df_veiculos.columns:
            clonados = df_veiculos[df_veiculos['clonado'] == 1]
            print(f"   ‚ö†Ô∏è Ve√≠culos clonados identificados: {len(clonados)}")
            return df_veiculos
        else:
            print("   ‚ö†Ô∏è Coluna 'clonado' n√£o encontrada no arquivo de ve√≠culos")
            return None
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao carregar informa√ß√µes de ve√≠culos: {e}")
        return None

def simular_dados_alpr():
    """
    Fun√ß√£o mantida por compatibilidade. Agora carrega dados reais do CSV.
    """
    print("üîÑ Carregando dados das passagens ALPR...")
    return carregar_dados_alpr("passagens.csv")

# =========================
# 2. Engenharia de Features
# =========================

def calcular_distancia_haversine(lat1, lon1, lat2, lon2):
    """
    Calcula a dist√¢ncia entre dois pontos usando a f√≥rmula de Haversine.
    
    Args:
        lat1, lon1: Coordenadas do primeiro ponto
        lat2, lon2: Coordenadas do segundo ponto
    
    Returns:
        float: Dist√¢ncia em quil√¥metros
    """
    return haversine((lat1, lon1), (lat2, lon2))

def calcular_velocidade_teorica(dist_km, tempo_segundos):
    """
    Calcula a velocidade te√≥rica necess√°ria para percorrer uma dist√¢ncia em um tempo.
    
    Args:
        dist_km: Dist√¢ncia em quil√¥metros
        tempo_segundos: Tempo em segundos
    
    Returns:
        float: Velocidade em km/h
    """
    if tempo_segundos <= 0:
        return 9999.0  # Velocidade imposs√≠vel para tempo zero ou negativo
    
    tempo_horas = tempo_segundos / 3600
    return dist_km / tempo_horas

def gerar_pares_features(df, df_veiculos=None, limiar_velocidade=150, max_pares_por_placa=50):
    """
    Gera pares de capturas da mesma placa em c√¢meras diferentes e extrai features.
    Para ve√≠culos clonados, cada passagem pode representar um clone diferente.
    
    Args:
        df: DataFrame com dados ALPR
        df_veiculos: DataFrame com informa√ß√µes dos ve√≠culos (incluindo flag clonado)
        limiar_velocidade: Velocidade limite para classificar como suspeito (km/h)
        max_pares_por_placa: Limite m√°ximo de pares por placa para evitar explos√£o combinat√≥ria
    
    Returns:
        tuple: (features_array, labels_array, pares_info)
    """
    features = []
    labels = []
    pares_info = []
    
    # Criar set de placas clonadas se informa√ß√£o dispon√≠vel
    placas_clonadas = set()
    if df_veiculos is not None and 'clonado' in df_veiculos.columns:
        placas_clonadas = set(df_veiculos[df_veiculos['clonado'] == 1]['placa'])
        print(f"üîç Placas clonadas identificadas: {len(placas_clonadas)}")
    
    placas_unicas = df['placa'].unique()
    print(f"üîç Analisando {len(placas_unicas):,} placas para gerar pares...")
    
    total_pares = 0
    placas_processadas = 0
    
    for placa in placas_unicas:
        eventos_placa = df[df['placa'] == placa].sort_values(by='ts').reset_index(drop=True)
        
        # Verificar se a placa √© conhecidamente clonada
        is_placa_clonada = placa in placas_clonadas
        
        # Limitar n√∫mero de eventos por placa para evitar explos√£o combinat√≥ria
        if len(eventos_placa) > 20:
            eventos_placa = eventos_placa.sample(n=20, random_state=42).sort_values(by='ts').reset_index(drop=True)
        
        pares_placa = 0
        
        if is_placa_clonada:
            # ESTRAT√âGIA PARA PLACAS CLONADAS:
            # Cada passagem pode ser de um clone diferente, ent√£o analisamos todos os pares
            print(f"   üö® Analisando placa clonada: {placa} ({len(eventos_placa)} passagens)")
            
            for i in range(len(eventos_placa)):
                if pares_placa >= max_pares_por_placa:
                    break
                    
                for j in range(i + 1, len(eventos_placa)):
                    if pares_placa >= max_pares_por_placa:
                        break
                        
                    evento1 = eventos_placa.iloc[i]
                    evento2 = eventos_placa.iloc[j]
                    
                    # Pular pares da mesma c√¢mera
                    if evento1['cam'] == evento2['cam']:
                        continue
                    
                    # Processar par e adicionar aos dados
                    if processar_par_eventos(evento1, evento2, placa, True, limiar_velocidade, 
                                           features, labels, pares_info):
                        pares_placa += 1
                        total_pares += 1
        else:
            # ESTRAT√âGIA PARA PLACAS NORMAIS:
            # Analisar principalmente pares consecutivos e alguns aleat√≥rios
            
            # 1. Pares consecutivos (comportamento normal esperado)
            for i in range(len(eventos_placa) - 1):
                if pares_placa >= max_pares_por_placa // 2:  # Reserva metade para consecutivos
                    break
                    
                evento1 = eventos_placa.iloc[i]
                evento2 = eventos_placa.iloc[i + 1]
                
                # Pular pares da mesma c√¢mera
                if evento1['cam'] == evento2['cam']:
                    continue
                
                # Processar par consecutivo
                if processar_par_eventos(evento1, evento2, placa, False, limiar_velocidade,
                                       features, labels, pares_info):
                    pares_placa += 1
                    total_pares += 1
            
            # 2. Alguns pares n√£o-consecutivos para detectar comportamentos an√¥malos
            if len(eventos_placa) > 2:
                import random
                pares_extras = min(max_pares_por_placa - pares_placa, len(eventos_placa) // 2)
                
                for _ in range(pares_extras):
                    if pares_placa >= max_pares_por_placa:
                        break
                        
                    # Escolher dois √≠ndices aleat√≥rios n√£o consecutivos
                    indices = list(range(len(eventos_placa)))
                    random.shuffle(indices)
                    
                    for i in range(len(indices) - 1):
                        for j in range(i + 1, len(indices)):
                            if abs(indices[i] - indices[j]) == 1:  # Pular consecutivos
                                continue
                                
                            evento1 = eventos_placa.iloc[indices[i]]
                            evento2 = eventos_placa.iloc[indices[j]]
                            
                            # Pular pares da mesma c√¢mera
                            if evento1['cam'] == evento2['cam']:
                                continue
                            
                            # Processar par n√£o-consecutivo
                            if processar_par_eventos(evento1, evento2, placa, False, limiar_velocidade,
                                                   features, labels, pares_info):
                                pares_placa += 1
                                total_pares += 1
                                break
                        if pares_placa >= max_pares_por_placa:
                            break
                    if pares_placa >= max_pares_por_placa:
                        break
        
        placas_processadas += 1
        if placas_processadas % 1000 == 0:
            print(f"   üìà Progresso: {placas_processadas:,}/{len(placas_unicas):,} placas "
                  f"({total_pares:,} pares gerados)")
    
    features_array = np.array(features)
    labels_array = np.array(labels)
    
    print(f"üìä Gerados {len(features):,} pares de capturas")
    print(f"   üö® Suspeitos: {sum(labels):,} ({sum(labels)/len(labels)*100:.1f}%)")
    print(f"   ‚úÖ Normais: {len(labels) - sum(labels):,} ({(len(labels) - sum(labels))/len(labels)*100:.1f}%)")
    
    # Estat√≠sticas por velocidade
    velocidades = [par['velocidade_kmh'] for par in pares_info]
    print(f"   üìä Velocidade m√©dia: {np.mean(velocidades):.1f} km/h")
    print(f"   üìä Velocidade m√°xima: {np.max(velocidades):.1f} km/h")
    print(f"   üìä Velocidades > 200 km/h: {sum(1 for v in velocidades if v > 200):,}")
    
    return features_array, labels_array, pares_info


def processar_par_eventos(evento1, evento2, placa, is_placa_clonada, limiar_velocidade, 
                         features, labels, pares_info):
    """
    Processa um par de eventos e adiciona √†s listas de features/labels.
    
    Args:
        evento1, evento2: Eventos a serem comparados
        placa: Placa do ve√≠culo
        is_placa_clonada: Se a placa √© conhecidamente clonada
        limiar_velocidade: Limite de velocidade para classificar como suspeito
        features, labels, pares_info: Listas para adicionar os resultados
        
    Returns:
        bool: True se o par foi processado com sucesso
    """
    # Calcular features
    dist_km = calcular_distancia_haversine(
        evento1['lat'], evento1['lon'],
        evento2['lat'], evento2['lon']
    )
    
    delta_t_ms = abs(evento2['ts'] - evento1['ts'])
    delta_t_segundos = delta_t_ms / 1000
    
    # Pular pares com tempo muito pequeno (menos de 30 segundos)
    if delta_t_segundos < 30:
        return False
    
    velocidade_teorica = calcular_velocidade_teorica(dist_km, delta_t_segundos)
    
    # Features para o modelo
    feature_vector = [dist_km, delta_t_segundos, velocidade_teorica]
    features.append(feature_vector)
    
    # Determinar label baseado na l√≥gica de clonagem
    if is_placa_clonada:
        # Para placas conhecidamente clonadas, usar crit√©rios mais rigorosos
        # Velocidade alta OU dist√¢ncia grande em tempo curto
        label = 1 if (velocidade_teorica > limiar_velocidade or 
                     (dist_km > 50 and delta_t_segundos < 3600)) else 0
    else:
        # Para placas normais, usar apenas velocidade como crit√©rio principal
        label = 1 if velocidade_teorica > limiar_velocidade else 0
    
    labels.append(label)
    
    # Informa√ß√µes do par para debugging
    par_info = {
        'placa': placa,
        'cam1': evento1['cam'],
        'cam2': evento2['cam'],
        'ts1': evento1['ts'],
        'ts2': evento2['ts'],
        'dist_km': round(dist_km, 2),
        'delta_t_segundos': round(delta_t_segundos, 0),
        'velocidade_kmh': round(velocidade_teorica, 1),
        'suspeito': label == 1,
        'placa_clonada': is_placa_clonada,
        'timestamp1_legivel': pd.to_datetime(evento1['ts'], unit='ms').strftime('%Y-%m-%d %H:%M:%S'),
        'timestamp2_legivel': pd.to_datetime(evento2['ts'], unit='ms').strftime('%Y-%m-%d %H:%M:%S'),
        'tipo_par': 'clonado' if is_placa_clonada else 'normal'
    }
    pares_info.append(par_info)
    
    return True

# =========================
# 3. Treinamento e Avalia√ß√£o do Modelo
# =========================

def treinar_modelo_random_forest(X, y, test_size=0.3, random_state=42):
    """
    Treina um modelo Random Forest para classifica√ß√£o de ve√≠culos clonados.
    
    Args:
        X: Array de features
        y: Array de labels
        test_size: Propor√ß√£o dos dados para teste
        random_state: Seed para reprodutibilidade
    
    Returns:
        tuple: (modelo_treinado, dados_teste)
    """
    print("ü§ñ Iniciando treinamento do modelo Random Forest...")
    
    # Verificar se temos dados suficientes
    if len(X) < 10:
        print("‚ö†Ô∏è Poucos dados para divis√£o treino/teste. Usando valida√ß√£o cruzada simples.")
        test_size = 0.2
    
    # Verificar balanceamento das classes
    unique, counts = np.unique(y, return_counts=True)
    print(f"   üìä Distribui√ß√£o das classes: {dict(zip(unique, counts))}")
    
    # Dividir dados
    stratify = y if len(unique) > 1 and min(counts) > 1 else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=stratify
    )
    
    # Configurar e treinar modelo
    modelo = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=random_state,
        class_weight='balanced',  # Para lidar com desbalanceamento
        min_samples_split=5,
        min_samples_leaf=2
    )
    
    modelo.fit(X_train, y_train)
    
    print(f"‚úÖ Modelo treinado com {len(X_train):,} amostras")
    print(f"   üìä Features: dist_km, delta_t_segundos, velocidade_teorica")
    print(f"   ‚öñÔ∏è Classes balanceadas automaticamente")
    
    return modelo, (X_train, X_test, y_train, y_test)

def avaliar_modelo(modelo, dados_teste):
    """
    Avalia o desempenho do modelo com m√©tricas detalhadas.
    
    Args:
        modelo: Modelo treinado
        dados_teste: Tupla com (X_train, X_test, y_train, y_test)
    """
    X_train, X_test, y_train, y_test = dados_teste
    
    print("\n" + "="*50)
    print("üìà AVALIA√á√ÉO DO MODELO")
    print("="*50)
    
    # Predi√ß√µes
    y_pred_train = modelo.predict(X_train)
    y_pred_test = modelo.predict(X_test)
    
    # M√©tricas de treino
    print("\nüéØ DESEMPENHO NO CONJUNTO DE TREINO:")
    print(f"   Accuracy: {accuracy_score(y_train, y_pred_train):.3f}")
    
    # M√©tricas de teste
    print("\nüéØ DESEMPENHO NO CONJUNTO DE TESTE:")
    print(f"   Accuracy:  {accuracy_score(y_test, y_pred_test):.3f}")
    print(f"   Precision: {precision_score(y_test, y_pred_test, average='weighted', zero_division=0):.3f}")
    print(f"   Recall:    {recall_score(y_test, y_pred_test, average='weighted', zero_division=0):.3f}")
    print(f"   F1-Score:  {f1_score(y_test, y_pred_test, average='weighted', zero_division=0):.3f}")
    
    # Matriz de confus√£o
    print("\nüìä MATRIZ DE CONFUS√ÉO:")
    cm = confusion_matrix(y_test, y_pred_test)
    print(cm)
    print("   [0,0]: Verdadeiros Negativos (Normal classificado como Normal)")
    print("   [0,1]: Falsos Positivos (Normal classificado como Suspeito)")  
    print("   [1,0]: Falsos Negativos (Suspeito classificado como Normal)")
    print("   [1,1]: Verdadeiros Positivos (Suspeito classificado como Suspeito)")
    
    # Relat√≥rio de classifica√ß√£o detalhado
    print("\nüìã RELAT√ìRIO DE CLASSIFICA√á√ÉO:")
    print(classification_report(y_test, y_pred_test, target_names=['Normal', 'Suspeito'], zero_division=0))
    
    # Import√¢ncia das features
    print("\nüîç IMPORT√ÇNCIA DAS FEATURES:")
    feature_names = ['Dist√¢ncia (km)', 'Tempo (seg)', 'Velocidade (km/h)']
    importancias = modelo.feature_importances_
    for nome, importancia in zip(feature_names, importancias):
        print(f"   {nome}: {importancia:.3f}")

# =========================
# 4. Fun√ß√£o de Predi√ß√£o para Novos Eventos
# =========================

def prever_evento(modelo, captura1, captura2, mostrar_detalhes=True):
    """
    Prev√™ se um par de capturas indica poss√≠vel clonagem de ve√≠culo.
    
    Args:
        modelo: Modelo Random Forest treinado
        captura1: Dict com dados da primeira captura {'lat', 'lon', 'ts'}
        captura2: Dict com dados da segunda captura {'lat', 'lon', 'ts'}
        mostrar_detalhes: Se deve imprimir detalhes do c√°lculo
    
    Returns:
        tuple: (predicao, probabilidade, detalhes)
    """
    # Calcular features
    dist_km = calcular_distancia_haversine(
        captura1['lat'], captura1['lon'],
        captura2['lat'], captura2['lon']
    )
    
    delta_t_ms = abs(captura2['ts'] - captura1['ts'])
    delta_t_segundos = delta_t_ms / 1000
    
    velocidade_teorica = calcular_velocidade_teorica(dist_km, delta_t_segundos)
    
    # Preparar entrada para o modelo
    features_entrada = np.array([[dist_km, delta_t_segundos, velocidade_teorica]])
    
    # Fazer predi√ß√£o
    predicao = modelo.predict(features_entrada)[0]
    probabilidades = modelo.predict_proba(features_entrada)[0]
    probabilidade_suspeito = probabilidades[1] if len(probabilidades) > 1 else 0.0
    
    # Detalhes do c√°lculo
    detalhes = {
        'distancia_km': dist_km,
        'tempo_segundos': delta_t_segundos,
        'velocidade_kmh': velocidade_teorica,
        'predicao': predicao,
        'probabilidade_suspeito': probabilidade_suspeito
    }
    
    if mostrar_detalhes:
        print(f"\nüîç AN√ÅLISE DO PAR DE CAPTURAS:")
        print(f"   Dist√¢ncia: {dist_km:.2f} km")
        print(f"   Tempo: {delta_t_segundos:.0f} segundos ({delta_t_segundos/60:.1f} minutos)")
        print(f"   Velocidade te√≥rica: {velocidade_teorica:.1f} km/h")
        print(f"   Predi√ß√£o: {'SUSPEITO' if predicao == 1 else 'NORMAL'}")
        print(f"   Probabilidade de clonagem: {probabilidade_suspeito:.2f}")
        print(f"   Alerta: {'üö® SIM' if predicao == 1 else '‚úÖ N√ÉO'}")
    
    return predicao, probabilidade_suspeito, detalhes

# =========================
# 5. Fun√ß√£o Principal e Exemplos de Uso
# =========================

def main():
    """
    Fun√ß√£o principal que executa todo o pipeline de detec√ß√£o de ve√≠culos clonados.
    """
    print("üöó SISTEMA DE DETEC√á√ÉO DE VE√çCULOS CLONADOS")
    print("=" * 60)
    
    # 1. Carregar dados das passagens
    df = carregar_dados_alpr("passagens.csv")
    if df is None or len(df) == 0:
        print("‚ùå N√£o foi poss√≠vel carregar dados de passagens. Encerrando.")
        return None, None, None
    
    # 2. Carregar informa√ß√µes dos ve√≠culos (opcional)
    df_veiculos = carregar_veiculos_info("veiculos_gerados.csv")
    
    # 3. Gerar features e labels
    print("\nüîß Gerando features dos pares de capturas...")
    X, y, pares_info = gerar_pares_features(df, df_veiculos, limiar_velocidade=150)
    
    if len(X) == 0:
        print("‚ùå Nenhum par de capturas gerado. Verifique os dados.")
        return None, None, None
    
    # 4. Treinar modelo
    modelo, dados_teste = treinar_modelo_random_forest(X, y)
    
    # 5. Avaliar modelo
    avaliar_modelo(modelo, dados_teste)
    
    # 6. Mostrar alguns exemplos dos pares analisados
    print("\n" + "="*60)
    print("üìã EXEMPLOS DE PARES ANALISADOS:")
    print("="*60)
    
    # Ordenar por velocidade decrescente para mostrar casos mais suspeitos
    pares_ordenados = sorted(pares_info, key=lambda x: x['velocidade_kmh'], reverse=True)
    
    print("\nüö® TOP 5 CASOS MAIS SUSPEITOS (maior velocidade):")
    for i, par in enumerate(pares_ordenados[:5]):
        status = "üö® SUSPEITO" if par['suspeito'] else "‚úÖ NORMAL"
        clonado_info = f" ({'CLONADO' if par['placa_clonada'] else 'NORMAL'})" if 'placa_clonada' in par else ""
        print(f"\n{i+1}. Placa {par['placa']}{clonado_info} - {status}")
        print(f"   {par['cam1']} ‚Üí {par['cam2']}")
        print(f"   {par['timestamp1_legivel']} ‚Üí {par['timestamp2_legivel']}")
        print(f"   Dist√¢ncia: {par['dist_km']:.2f} km")
        print(f"   Tempo: {par['delta_t_segundos']:.0f}s ({par['delta_t_segundos']/60:.1f} min)")
        print(f"   Velocidade: {par['velocidade_kmh']:.1f} km/h")
    
    # 7. Exemplos de predi√ß√£o para novos eventos
    print("\n" + "="*60)
    print("üîÆ EXEMPLOS DE PREDI√á√ÉO PARA NOVOS EVENTOS:")
    print("="*60)
    
    # Usar timestamps realistas baseados nos dados carregados
    timestamp_base = int(df['ts'].min())
    
    # Exemplo 1: Caso suspeito (velocidade alta)
    print("\n1Ô∏è‚É£ EXEMPLO SUSPEITO:")
    evento1 = {"lat": -27.5954, "lon": -48.5480, "ts": timestamp_base}
    evento2 = {"lat": -27.6954, "lon": -48.5580, "ts": timestamp_base + 200000}  # 200 segundos depois
    prever_evento(modelo, evento1, evento2)
    
    # Exemplo 2: Caso normal (velocidade aceit√°vel)
    print("\n2Ô∏è‚É£ EXEMPLO NORMAL:")
    evento3 = {"lat": -27.5954, "lon": -48.5480, "ts": timestamp_base}
    evento4 = {"lat": -27.6000, "lon": -48.5490, "ts": timestamp_base + 1800000}  # 30 minutos depois
    prever_evento(modelo, evento3, evento4)
    
    # Exemplo 3: Caso lim√≠trofe
    print("\n3Ô∏è‚É£ EXEMPLO LIM√çTROFE:")
    evento5 = {"lat": -27.5954, "lon": -48.5480, "ts": timestamp_base}
    evento6 = {"lat": -27.6200, "lon": -48.5600, "ts": timestamp_base + 600000}  # 10 minutos depois
    prever_evento(modelo, evento5, evento6)
    
    print("\n" + "="*60)
    print("‚úÖ AN√ÅLISE CONCLU√çDA!")
    print("="*60)
    
    return modelo, df, pares_info

def carregar_dados_csv(caminho_arquivo):
    """
    Fun√ß√£o mantida por compatibilidade. Redireciona para carregar_dados_alpr.
    """
    return carregar_dados_alpr(caminho_arquivo)

def salvar_resultados_csv(pares_info, caminho_saida="resultados_analise.csv"):
    """
    Salva os resultados da an√°lise em um arquivo CSV.
    
    Args:
        pares_info: Lista com informa√ß√µes dos pares analisados
        caminho_saida: Caminho do arquivo de sa√≠da
    """
    try:
        df_resultados = pd.DataFrame(pares_info)
        df_resultados.to_csv(caminho_saida, index=False, encoding='utf-8')
        print(f"üíæ Resultados salvos em: {caminho_saida}")
        
        # Estat√≠sticas do arquivo salvo
        suspeitos = len(df_resultados[df_resultados['suspeito'] == True])
        print(f"   üìä Total de pares: {len(df_resultados):,}")
        print(f"   üö® Casos suspeitos: {suspeitos:,} ({suspeitos/len(df_resultados)*100:.1f}%)")
        
    except Exception as e:
        print(f"‚ùå Erro ao salvar resultados: {e}")

# =========================
# 6. Execu√ß√£o do Programa
# =========================

if __name__ == "__main__":
    # Executar an√°lise completa
    resultado = main()
    
    if resultado[0] is not None:  # Se o modelo foi treinado com sucesso
        modelo_treinado, dados_originais, resultados_pares = resultado
        
        # Salvar resultados (opcional)
        salvar_resultados_csv(resultados_pares, "analise_clonagem_veiculos.csv")
        
        print("\nüí° DICAS DE USO:")
        print("- O programa agora l√™ dados do arquivo 'passagens.csv'")
        print("- Para usar outros arquivos: carregar_dados_alpr('seu_arquivo.csv')")
        print("- Para predi√ß√µes individuais: prever_evento(modelo, evento1, evento2)")
        print("- Ajuste o limiar de velocidade conforme necess√°rio (padr√£o: 150 km/h)")
        print("- Os resultados s√£o salvos em 'analise_clonagem_veiculos.csv'")
    else:
        print("‚ùå N√£o foi poss√≠vel executar a an√°lise. Verifique os arquivos de entrada.")

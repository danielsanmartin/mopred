"""
Teste de Modelos com Mudan√ßas Temporais
=======================================

Script principal para comparar Random Forest tradicional vs. adaptativo.
"""

from simulador_streaming_alpr import SimuladorStreamingALPR
from comparador_modelos import ComparadorModelos
import json
import pandas as pd
import numpy as np
import hashlib
import hmac
from haversine import haversine
from joblib import Parallel, delayed
from utils import processar_placa_basico
try:
    from imblearn.over_sampling import SMOTE
except ImportError:
    SMOTE = None

def pseudonimizar_placa(placa, salt="mopred_2024_salt_key"):
    """
    Gera um pseud√¥nimo irrevers√≠vel para a placa usando SHA-256.
    
    Args:
        placa: Placa original (ex: "ABC1234")
        salt: Chave secreta para tornar o hash mais seguro
    
    Returns:
        String pseudonimizada (ex: "PSEUDO_A1B2C3D4")
    """
    # Normalizar placa (remover espa√ßos, converter para mai√∫scula)
    placa_normalizada = placa.upper().replace(" ", "").replace("-", "")
    
    # Gerar hash seguro com salt
    hash_obj = hmac.new(
        salt.encode('utf-8'), 
        placa_normalizada.encode('utf-8'), 
        hashlib.sha256
    )
    hash_hex = hash_obj.hexdigest()
    
    # Usar apenas os primeiros 8 caracteres para pseud√¥nimo mais leg√≠vel
    pseudonimo = f"PSEUDO_{hash_hex[:8].upper()}"
    
    return pseudonimo

def aplicar_pseudonimizacao_eventos(eventos, config=None):
    """
    Aplica pseudonimiza√ß√£o em todos os eventos.
    
    Args:
        eventos: Lista de eventos com placas originais
        config: Configura√ß√£o com salt personalizado
    
    Returns:
        Lista de eventos com placas pseudonimizadas
    """
    print("üîí Aplicando pseudonimiza√ß√£o nas placas...")
    
    # Usar salt do config ou padr√£o
    salt = config.get("salt_pseudonimizacao", "mopred_doutorado_2024_chave_secreta") if config else "mopred_doutorado_2024_chave_secreta"
    
    placas_mapeadas = {}
    total_pseudonimizadas = 0
    
    for evento in eventos:
        placa_original = evento.placa
        
        # Verificar se j√° foi pseudonimizada
        if placa_original not in placas_mapeadas:
            placa_pseudo = pseudonimizar_placa(placa_original, salt)
            placas_mapeadas[placa_original] = placa_pseudo
            total_pseudonimizadas += 1
        
        # Substituir placa original pela pseudonimizada
        evento.placa = placas_mapeadas[placa_original]
    
    print(f"‚úÖ {total_pseudonimizadas} placas √∫nicas pseudonimizadas")
    print(f"üìä Total de eventos processados: {len(eventos)}")
    
    return eventos

def simular_mudancas_temporais(simulador, config):
    """Simula mudan√ßas nos padr√µes de clonagem ao longo do tempo."""
    print("üîÑ Simulando mudan√ßas temporais nos dados...")
    
    if not simulador.eventos_gerados:
        print("‚ùå Nenhum evento gerado para simular mudan√ßas")
        return []
    
    # Dividir eventos em fases com caracter√≠sticas diferentes
    todos_eventos = simulador.eventos_gerados.copy()
    
    # Fase 1: Comportamento normal (primeiros 33%)
    fase1_size = len(todos_eventos) // 3
    fase1_eventos = todos_eventos[:fase1_size]
    
    # Fase 2: Aumento de atividade clonada (33%-66%)
    fase2_eventos = todos_eventos[fase1_size:2*fase1_size]
    # Artificialmente aumentar eventos clonados
    contador = 0
    for evento in fase2_eventos:
        contador += 1
        if contador % 5 == 0:  # A cada 5 eventos
            evento.is_clonado = True
    
    # Fase 3: Novos padr√µes de clonagem (√∫ltimos 33%)
    fase3_eventos = todos_eventos[2*fase1_size:]
    
    # Simular novos padr√µes geogr√°ficos usando coordenadas do config
    print("  üìç Criando zonas quentes baseadas na configura√ß√£o...")
    
    # Selecionar algumas cidades como zonas quentes (cidades maiores ou estrat√©gicas)
    cidades_config = config.get("lat_lon_por_cidade", {})
    zonas_quentes = config.get("zonas_quentes", [])  # Cidades estrat√©gicas
    raio_zona = config.get("raio_zonas_quentes", 0.2)  # 0.2 equivale a aproximadamente 20km

    total_clonados_zona = 0
    for evento in fase3_eventos:
        lat, lon = evento.lat, evento.lon
        
        # Verificar se est√° pr√≥ximo a alguma zona quente
        for cidade in zonas_quentes:
            if cidade in cidades_config:
                cidade_lat, cidade_lon = cidades_config[cidade]
                # Verificar se est√° dentro do raio da cidade
                if (abs(lat - cidade_lat) < raio_zona and 
                    abs(lon - cidade_lon) < raio_zona):
                    evento.is_clonado = True
                    total_clonados_zona += 1
                    break
    
    print(f"  ‚úÖ {total_clonados_zona} eventos marcados como clonados nas zonas quentes")
    
    print(f"‚úÖ Fases criadas:")
    print(f"  Fase 1: {len(fase1_eventos):,} eventos (baseline)")
    print(f"  Fase 2: {len(fase2_eventos):,} eventos (+ clonagem)")
    print(f"  Fase 3: {len(fase3_eventos):,} eventos (novos padr√µes)")
    
    return [fase1_eventos, fase2_eventos, fase3_eventos]

def preparar_dados_treino_inicial(fase1_eventos, n_jobs, config=None):
    print(f"\nüå± PREPARANDO TREINO INICIAL...")

    if not fase1_eventos:
        return np.array([]), np.array([]), np.array([])

    eventos_dict = [evento.to_dict() for evento in fase1_eventos]
    df_treino = pd.DataFrame(eventos_dict)

    if len(df_treino) == 0:
        return np.array([]), np.array([]), np.array([])

    placas_unicas = df_treino['placa'].unique()

    # Usar a nova fun√ß√£o de processamento b√°sico para treino inicial
    resultados = Parallel(n_jobs=n_jobs, backend="multiprocessing")(
        delayed(processar_placa_basico)(df_treino[df_treino['placa'] == placa].sort_values('timestamp'))
        for placa in placas_unicas
    )

    features, labels = [], []
    features_multimodal = []  # Para armazenar features com 5 dimens√µes
    resultados_validos = 0
    total_features_ignorados = 0
    total_labels_ignorados = 0
    for r in resultados:
        if isinstance(r, (list, tuple)) and len(r) >= 3:
            f, l, f_semelhanca = r[:3]
            if len(f) == len(l) and len(f_semelhanca) == len(l):
                for feat, label, feat_mm in zip(f, l, f_semelhanca):
                    # Classifica√ß√£o bin√°ria:
                    # 0: n√£o clonado
                    # 1: clonado (id√™ntico ou n√£o id√™ntico)
                    if label == 0:
                        labels.append(0)
                    else:
                        labels.append(1)
                    features.append(feat)
                    features_multimodal.append(feat_mm)
                resultados_validos += 1
            else:
                print(f"‚ö†Ô∏è Inconsist√™ncia: {len(f)} features vs {len(l)} labels ignorados para uma placa.")
                total_features_ignorados += len(f)
                total_labels_ignorados += len(l)
        else:
            print(f"‚ö†Ô∏è Resultado inv√°lido ignorado: {r}")
    if resultados_validos < len(resultados):
        print(f"‚ö†Ô∏è {len(resultados) - resultados_validos} resultados ignorados por formato inesperado.")
    if total_features_ignorados > 0 or total_labels_ignorados > 0:
        print(f"‚ö†Ô∏è Total ignorado por inconsist√™ncia: {total_features_ignorados} features, {total_labels_ignorados} labels.")

    X_treino = np.array(features, dtype=np.float32)
    X_treino_multimodal = np.array(features_multimodal, dtype=np.float32)  # Features com 5 dimens√µes
    y_treino = np.array(labels, dtype=np.int32).ravel()

    # Verificar se deve usar SMOTE
    usar_smote = config.get("usar_smote", True) if config else True
    
    if usar_smote:
        return balancear_com_smote_binario(X_treino, X_treino_multimodal, y_treino, features_multimodal, config)
    else:
        print("‚ö†Ô∏è SMOTE desabilitado pela configura√ß√£o.")
        print(f"‚úÖ Dados de treino preparados (sem balanceamento):")
        print(f"   üìä {len(X_treino):,} pares de eventos")
        for classe in [0, 1]:
            print(f"   Classe {classe}: {np.sum(y_treino == classe)} exemplos")
        if len(features_multimodal) > 0:
            print(f"   üß¨ M√©dia de semelhan√ßa: {np.mean([f[-1] for f in features_multimodal]):.3f}")
        else:
            print(f"   üß¨ M√©dia de semelhan√ßa: N/A")
        return X_treino, X_treino_multimodal, y_treino

def interpretar_impacto_shap_dinamico(valor_shap, todos_shap_values):
    abs_valor = abs(valor_shap)
    abs_todos = [abs(v) for v in todos_shap_values]
    
    # Usar thresholds mais baixos para capturar melhor as nuances
    p60 = np.percentile(abs_todos, 60)  # Mais baixo que P75
    p30 = np.percentile(abs_todos, 30)  # Mais baixo que P25
    
    # Determinar magnitude do impacto
    if abs_valor >= p60:
        nivel = "Alto impacto"
        emoji = "üî¥"  # Vermelho
    elif abs_valor >= p30:
        nivel = "Impacto moderado"  
        emoji = "üü°"  # Amarelo
    else:
        nivel = "Baixo impacto"
        emoji = "üü¢"  # Verde
    
    # Adicionar dire√ß√£o (+ ou -)
    if valor_shap >= 0:
        return f"{emoji} + {nivel}"  # Aumenta probabilidade
    else:
        return f"{emoji} - {nivel}"  # Diminui probabilidade

def gerar_relatorio_xai_shap(modelo_rf, eventos, titulo_relatorio, max_casos_exibidos=5):
    """
    Gera e imprime relat√≥rio de explicabilidade SHAP para casos clonados.
    
    Args:
        modelo_rf: Modelo Random Forest treinado
        eventos: Lista de eventos para an√°lise
        titulo_relatorio: T√≠tulo do relat√≥rio a ser exibido
        max_casos_exibidos: N√∫mero m√°ximo de casos clonados a serem exibidos (padr√£o: 5)
    """
    try:
        import shap
    except ImportError:
        print("Instalando pacote SHAP...")
        import subprocess; subprocess.check_call(["pip", "install", "shap"])
        import shap
    
    print(f"\n{titulo_relatorio}")
    
    if not eventos:
        print("Sem eventos para explica√ß√£o XAI.")
        return
    
    # Preparar dados de teste
    eventos_dict = [evento.to_dict() for evento in eventos]
    df_teste = pd.DataFrame(eventos_dict)
    features = []
    placas_unicas = df_teste['placa'].unique()
    
    for placa in placas_unicas:
        eventos_placa = df_teste[df_teste['placa'] == placa].sort_values('timestamp')
        if len(eventos_placa) < 2:
            continue
        for i in range(len(eventos_placa) - 1):
            evento1 = eventos_placa.iloc[i]
            evento2 = eventos_placa.iloc[i + 1]
            if evento1['cam'] == evento2['cam']:
                continue
            dist_km = haversine((evento1['lat'], evento1['lon']), (evento2['lat'], evento2['lon']))
            delta_t_ms = abs(evento2['timestamp'] - evento1['timestamp'])
            delta_t_segundos = delta_t_ms / 1000
            if delta_t_segundos < 30:
                continue
            velocidade_kmh = (dist_km / (delta_t_segundos / 3600)) if delta_t_segundos > 0 else 9999
            num_infracoes = evento1.get('num_infracoes', 0)
            
            # Decompor semelhan√ßa em 3 features booleanas
            marca_modelo_igual = 1.0 if (evento1.get('marca') == evento2.get('marca') and 
                                         evento1.get('modelo') == evento2.get('modelo')) else 0.0
            tipo_igual = 1.0 if evento1.get('tipo') == evento2.get('tipo') else 0.0
            cor_igual = 1.0 if evento1.get('cor') == evento2.get('cor') else 0.0
            
            feature_vector = [dist_km, delta_t_segundos, velocidade_kmh, num_infracoes, 
                            marca_modelo_igual, tipo_igual, cor_igual]
            features.append(feature_vector)
    
    X_teste = np.array(features)
    if len(X_teste) == 0:
        print("Sem dados suficientes para explica√ß√£o XAI.")
        return
    
    # Gerar explica√ß√µes SHAP
    explainer = shap.TreeExplainer(modelo_rf)
    shap_values = explainer.shap_values(X_teste)
    
    # Import√¢ncia global das features
    feature_names = ["dist_km", "delta_t_segundos", "velocidade_kmh", "num_infracoes", "marca_modelo_igual", "tipo_igual", "cor_igual"]
    print("Import√¢ncia global das features:")
    n_features_modelo = len(modelo_rf.feature_importances_)
    for i in range(min(len(feature_names), n_features_modelo)):
        nome = feature_names[i]
        print(f"  {nome}: {modelo_rf.feature_importances_[i]:.3f}")
    
    # Relat√≥rio SHAP para casos clonados
    print("\nExemplo de explica√ß√£o SHAP para os primeiros casos clonados (formato amig√°vel):")
    preds = modelo_rf.predict(X_teste)
    probs = modelo_rf.predict_proba(X_teste)
    traducao = {
        "dist_km": "Dist√¢ncia entre c√¢meras (km)",
        "delta_t_segundos": "Tempo entre leituras (segundos)",
        "velocidade_kmh": "Velocidade estimada (km/h)",
        "num_infracoes": "N√∫mero de infra√ß√µes",
        "marca_modelo_igual": "Marca/modelo iguais",
        "tipo_igual": "Tipo igual",
        "cor_igual": "Cor igual"
    }
    
    clonados_exibidos = 0
    col_widths = [38, 12, 12, 22]
    
    def fmt_cell(val, width, align='center'):
        val_str = str(val)
        if align == 'center':
            return val_str.center(width)
        elif align == 'right':
            return val_str.rjust(width)
        else:
            return val_str.ljust(width)
    
    header = ["Caracter√≠stica", "Valor", "Impacto SHAP", "Interpreta√ß√£o"]
    print("|" + "|".join([fmt_cell(h, w) for h, w in zip(header, col_widths)]) + "|")
    print("|" + "|".join(["-"*w for w in col_widths]) + "|")
    
    total_clonados = np.sum(preds == 1)
    for idx in range(len(X_teste)):
        if preds[idx] != 1:
            continue
        clonados_exibidos += 1
        if clonados_exibidos > max_casos_exibidos:
            break
        
        # Calcular escore de suspei√ß√£o (probabilidade da classe "clonado")
        escore_suspeicao = probs[idx][1] * 100  # Probabilidade da classe 1 (clonado) em porcentagem
        print(f"\nCaso {idx+1} - Predi√ß√£o: CLONADO - Escore de suspei√ß√£o: {escore_suspeicao:.2f}%")
        print("|" + "|".join([fmt_cell(h, w) for h, w in zip(header, col_widths)]) + "|")
        print("|" + "|".join(["-"*w for w in col_widths]) + "|")
        
        shap_vals = shap_values[1][idx] if isinstance(shap_values, list) else shap_values[idx]
        feat_vals = X_teste[idx]
        resumo = []
        
        # Usar interpreta√ß√£o din√¢mica baseada na distribui√ß√£o dos valores SHAP deste caso
        for i, nome in enumerate(feature_names):
            nome_pt = traducao.get(nome, nome)
            valor = f"{feat_vals[i]:.3f}"
            impacto_raw = shap_vals[i]
            if isinstance(impacto_raw, (np.ndarray, list)):
                impacto_val = impacto_raw[1] if len(impacto_raw) > 1 else impacto_raw[0]
            else:
                impacto_val = impacto_raw
            impacto_fmt = f"{impacto_val:.3f}"
            
            # Usar interpreta√ß√£o din√¢mica
            interpretacao = interpretar_impacto_shap_dinamico(impacto_val, shap_vals)
            # Extrai emoji e texto sem cortar a 1¬™ letra
            partes = interpretacao.split(' ', 1)
            emoji = partes[0] if partes else ''
            interpret = partes[1] if len(partes) > 1 else ''
            
            # Gerar resumo baseado na interpreta√ß√£o din√¢mica
            if "Alto impacto" in interpretacao:
                if nome == "num_infracoes":
                    resumo.append("O n√∫mero de infra√ß√µes teve forte influ√™ncia na decis√£o de clonagem.")
                elif nome in ["marca_modelo_igual", "tipo_igual", "cor_igual"]:
                    resumo.append(f"A similaridade de {nome_pt.lower()} teve forte influ√™ncia na decis√£o de clonagem.")
                else:
                    resumo.append(f"{nome_pt} teve forte influ√™ncia na decis√£o.")
            elif "Impacto moderado" in interpretacao:
                if nome == "num_infracoes":
                    resumo.append("O n√∫mero de infra√ß√µes teve impacto moderado na decis√£o.")
                elif nome in ["marca_modelo_igual", "tipo_igual", "cor_igual"]:
                    resumo.append(f"A similaridade de {nome_pt.lower()} teve impacto moderado na decis√£o.")
            
            print("|" +
                  fmt_cell(nome_pt, col_widths[0], 'left') + "|" +
                  fmt_cell(valor, col_widths[1], 'right') + "|" +
                  fmt_cell(impacto_fmt, col_widths[2], 'right') + "|" +
                  fmt_cell(f"{emoji} {interpret}", col_widths[3], 'left') + "|")
        
        if resumo:
            print("Resumo: " + " ".join(resumo))
        else:
            print("Resumo: Nenhuma caracter√≠stica teve impacto relevante para indicar clonagem.")
    
    if clonados_exibidos == 0:
        print(f"Nenhum caso clonado foi exibido. Total de predi√ß√µes clonadas: {total_clonados}")

def balancear_com_smote_binario(X_treino, X_treino_multimodal, y_treino, features_multimodal, config=None):
    """
    Aplica balanceamento SMOTE bin√°rio nos dados de treino.
    Retorna os dados balanceados ou originais se n√£o for poss√≠vel balancear.
    """
    # Calcular n√∫mero de amostras por classe
    if SMOTE is None:
        print("‚ö†Ô∏è imbalanced-learn n√£o instalado. Dados n√£o balanceados.")
        print(f"‚úÖ Dados de treino preparados:")
        print(f"   üìä {len(X_treino):,} pares de eventos")
        for classe in [0, 1]:
            print(f"   Classe {classe}: {np.sum(y_treino == classe)} exemplos")
        if len(features_multimodal) > 0:
            print(f"   üß¨ M√©dia de semelhan√ßa: {np.mean([f[-1] for f in features_multimodal]):.3f}")
        else:
            print(f"   üß¨ M√©dia de semelhan√ßa: N/A")
        return X_treino, X_treino_multimodal, y_treino
    unique, counts = np.unique(y_treino, return_counts=True)
    min_class_count = np.min(counts) if len(counts) > 0 else 0
    # S√≥ aplica SMOTE se todas as classes tiverem pelo menos 2 exemplos
    if np.all(counts >= 2):
        # Usar k_neighbors do config ou valor padr√£o
        k_neighbors_config = config.get("smote_k_neighbors", 5) if config else 5
        k_neighbors = max(1, min(min_class_count - 1, k_neighbors_config))
        
        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
        X_treino_bal, y_treino_bal = smote.fit_resample(X_treino, y_treino)
        X_treino_multimodal_bal, y_treino_multimodal_bal = smote.fit_resample(X_treino_multimodal, y_treino)
        print(f"‚úÖ Dados balanceados com SMOTE (k_neighbors={k_neighbors}):")
        print(f"   üìä {len(X_treino_bal):,} pares de eventos (tradicional)")
        print(f"   üìä {len(X_treino_multimodal_bal):,} pares de eventos (multimodal)")
        # Contagem por classe
        for classe in [0, 1]:
            print(f"   Classe {classe}: {np.sum(y_treino_bal == classe)} exemplos")
        if len(X_treino_multimodal_bal) > 0:
            print(f"   üß¨ M√©dia de semelhan√ßa: {np.mean([f[-1] for f in X_treino_multimodal_bal]):.3f}")
        else:
            print(f"   üß¨ M√©dia de semelhan√ßa: N/A")
        return X_treino_bal, X_treino_multimodal_bal, y_treino_bal
    else:
        print("‚ö†Ô∏è SMOTE n√£o aplicado: pelo menos uma classe tem menos de 2 exemplos.")
        print(f"‚úÖ Dados de treino preparados (sem balanceamento):")
        print(f"   üìä {len(X_treino):,} pares de eventos")
        for classe in [0, 1]:
            print(f"   Classe {classe}: {np.sum(y_treino == classe)} exemplos")
        if len(features_multimodal) > 0:
            print(f"   üß¨ M√©dia de semelhan√ßa: {np.mean([f[-1] for f in features_multimodal]):.3f}")
        else:
            print(f"   üß¨ M√©dia de semelhan√ßa: N/A")
        return X_treino, X_treino_multimodal, y_treino

def main(config=None):
    """Fun√ß√£o principal do teste."""
    print("üöÄ INICIANDO TESTE DE MODELOS TEMPORAIS")
    print("=" * 60)
    
    # Se config n√£o foi passado, carregar do arquivo
    if config is None:
        try:
            with open("config.json", "r", encoding="utf-8") as f:
                config = json.load(f)
        except FileNotFoundError:
            print("‚ö†Ô∏è Arquivo config.json n√£o encontrado")
            config = {}
    

    try:
        # 1. Inicializar simulador
        simulador = SimuladorStreamingALPR("config.json")
        eventos = simulador.executar_simulacao_completa()

        if not eventos:
            print("‚ùå Nenhum evento gerado pelo simulador")
            return

        # 1b. Aplicar pseudonimiza√ß√£o se configurado
        if config and config.get("usar_pseudonimizacao", False):
            print("\nüîí APLICANDO PSEUDONIMIZA√á√ÉO...")
            eventos = aplicar_pseudonimizacao_eventos(eventos, config)
        else:
            print("‚ÑπÔ∏è Pseudonimiza√ß√£o desabilitada ou n√£o configurada")

        # 2. Simular mudan√ßas temporais
        fases = simular_mudancas_temporais(simulador, config)

        if not fases:
            print("‚ùå Erro ao criar fases temporais")
            return

        # 3. Preparar dados de treino inicial (fase 1)
        n_jobs = config.get("n_jobs", 8)
        X_treino, X_treino_multimodal, y_treino = preparar_dados_treino_inicial(fases[0], n_jobs, config)

        if len(X_treino) == 0:
            print("‚ùå Nenhum dado de treino gerado")
            return

        # 4. Inicializar comparador
        comparador = ComparadorModelos(simulador, n_jobs=n_jobs)

        # 5. Checagem de shapes antes do treino
        print(f"\nüîé Shape X_treino: {X_treino.shape}, Shape X_treino_multimodal: {X_treino_multimodal.shape}, Shape y_treino: {y_treino.shape}")
        if X_treino.shape[0] != y_treino.shape[0]:
            print(f"‚ùå Inconsist√™ncia: X_treino tem {X_treino.shape[0]} linhas, y_treino tem {y_treino.shape[0]} labels. Treino abortado.")
            return
        print("\nüåü Treinando modelo tradicional (apenas features b√°sicas)...")
        comparador.treinar_modelo_tradicional(X_treino, y_treino, multimodal=False)
        print("\nüåü Treinando modelo tradicional (multimodal: infra√ß√µes + semelhan√ßa)...")
        comparador.treinar_modelo_tradicional(X_treino_multimodal, y_treino, multimodal=True)

        # 6. Testar em janelas temporais
        print(f"\nüïí TESTANDO EM JANELAS TEMPORAIS...")

        janela_contador = 1

        for i, fase_eventos in enumerate(fases, 1):
            print(f"\n{'='*20} FASE {i} {'='*20}")

            if not fase_eventos:
                print(f"‚ö†Ô∏è Fase {i} vazia, pulando...")
                continue

            # Dividir fase em sub-janelas para an√°lise mais granular
            tamanho_subjanela = max(1, len(fase_eventos) // 3)

            for j in range(3):
                inicio_janela = j * tamanho_subjanela
                fim_janela = (j + 1) * tamanho_subjanela if j < 2 else len(fase_eventos)

                if inicio_janela >= len(fase_eventos):
                    break

                janela_eventos = fase_eventos[inicio_janela:fim_janela]

                if len(janela_eventos) < 10:  # M√≠nimo de eventos para an√°lise
                    print(f"‚ö†Ô∏è Janela {janela_contador} muito pequena ({len(janela_eventos)} eventos), pulando...")
                    janela_contador += 1
                    continue

                # Processar janela para modelo adaptativo (treino incremental)
                # Somente a partir da segunda janela (primeira √© baseline)
                if janela_contador > 1:
                    comparador.processar_janela_adaptativo(janela_eventos, multimodal=False)
                    comparador.processar_janela_adaptativo(janela_eventos, multimodal=True)

                # Avaliar todos os modelos e salvar m√©tricas para todos cen√°rios
                comparador.avaliar_janela(janela_eventos, janela_contador, multimodal=False)
                comparador.avaliar_janela(janela_eventos, janela_contador, multimodal=True)

                janela_contador += 1

        # 7. Gerar relat√≥rio final (inclui precis√£o, recall, f1 para todos cen√°rios)
        comparador.gerar_relatorio_final()

        # 7b. Explicabilidade XAI usando SHAP para o modelo tradicional
        try:
            import shap
        except ImportError:
            print("Instalando pacote SHAP...")
            import subprocess; subprocess.check_call(["pip", "install", "shap"])
            import shap

        modelo_rf = getattr(comparador, "modelo_tradicional_multimodal", None)
        if modelo_rf is not None:
            if len(fases) > 0 and len(fases[-1]) > 0:
                gerar_relatorio_xai_shap(modelo_rf, fases[-1], "üîé EXPLICABILIDADE (XAI) PARA RANDOM FOREST TRADICIONAL MULTIMODAL:")
            else:
                print("Sem dados de teste para explica√ß√£o XAI.")
        else:
            print("Modelo tradicional multimodal n√£o encontrado para XAI.")

        # 8. Teste adicional com janelas do simulador streaming
        print(f"\nü™ü TESTE ADICIONAL COM STREAMING POR JANELAS...")
        testar_streaming_janelas(simulador, comparador)

    except Exception as e:
        print(f"‚ùå Erro durante o teste: {e}")
        import traceback
        traceback.print_exc()

def testar_streaming_janelas(simulador, comparador):
    """Teste adicional usando o modo de janelas temporais do simulador."""
    try:
        print("üîÑ Reinicializando modelo adaptativo para teste limpo...")
        
        # Reinicializar modelo adaptativo para teste limpo
        from comparador_modelos import AdaptiveRandomForestWrapper
        comparador.modelo_adaptativo = AdaptiveRandomForestWrapper(
            n_models=10,
            seed=42
        )
        
        # Usar streaming por janelas (2 horas por janela para ter dados suficientes)
        janelas_stream = simulador.streaming_janelas_temporais(tamanho_janela_horas=2.0)
        
        janelas_processadas = 0
        janela_anterior = None
        
        for i, janela_eventos in enumerate(janelas_stream):
            if janelas_processadas >= 5:  # Testar apenas primeiras 5 janelas
                break
            
            print(f"\n--- Janela Streaming {i+1} ---")
            
            # Treinar modelo adaptativo com janela anterior
            if janela_anterior is not None and len(janela_anterior) > 10:
                print(f"üîÑ Treinando modelo adaptativo com janela anterior...")
                comparador.processar_janela_adaptativo(janela_anterior)
            
            # Avaliar na janela atual
            if len(janela_eventos) > 10:
                janela_numero = f"Stream-{i+1}"
                trad_metricas, adapt_metricas = comparador.avaliar_janela(janela_eventos, janela_numero)
                
                if trad_metricas is not None and adapt_metricas is not None:
                    janelas_processadas += 1
            else:
                print(f"‚ö†Ô∏è Janela {i+1} muito pequena para an√°lise")
            
            janela_anterior = janela_eventos
        
        print(f"‚úÖ Teste de streaming conclu√≠do com {janelas_processadas} janelas processadas")

        # Explicabilidade SHAP para casos clonados do streaming
        modelo_rf = getattr(comparador, "modelo_tradicional_multimodal", None)
        if modelo_rf is not None:
            eventos_streaming = []
            janelas_stream = simulador.streaming_janelas_temporais(tamanho_janela_horas=2.0)
            for janela_eventos in janelas_stream:
                eventos_streaming.extend(janela_eventos)
            if len(eventos_streaming) == 0:
                print("Sem eventos de streaming para explica√ß√£o XAI.")
                return
            gerar_relatorio_xai_shap(modelo_rf, eventos_streaming, "üîé EXPLICABILIDADE (XAI) PARA CASOS CLONADOS DO STREAMING (MULTIMODAL):")
        
    except Exception as e:
        print(f"‚ùå Erro no teste de streaming: {e}")

if __name__ == "__main__":
    print("üöÄ Iniciando an√°lise de modelos temporais para detec√ß√£o de clonagem")
    print("=" * 70)
    
    # Carregar configura√ß√£o
    config = None
    try:
        with open('config.json', 'r') as f:
            config = json.load(f)
    except FileNotFoundError:
        print("‚ö†Ô∏è Arquivo config.json n√£o encontrado. Usando configura√ß√£o padr√£o.")
        config = {
            "usar_smote": False,
            "smote_k_neighbors": 5,
            "usar_pseudonimizacao": False,
            "salt_pseudonimizacao": "mopred_doutorado_2024_chave_secreta"
        }
    
    main(config)
